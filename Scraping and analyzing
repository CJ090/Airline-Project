from selenium import webdriver
import time
from bs4 import BeautifulSoup
import csv
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
from nltk.stem import WordNetLemmatizer
import random
from nltk.classify import NaiveBayesClassifier
from nltk.classify import DecisionTreeClassifier
from nltk.classify import SklearnClassifier
from sklearn.svm import SVC
from nltk.classify import MaxentClassifier
from nltk.metrics.scores import (accuracy, precision, recall, f_measure)
from nltk.classify.util import accuracy
import collections
browser = webdriver.Firefox()
dataset = []
for i in range(0,4360,10):
    url =  'https://www.tripadvisor.com/Airline_Review-d8729177-Reviews-Cheap-Flights-or'+ str(i)
    browser.get(url)
    time.sleep(5)
    more_links = browser.find_elements_by_xpath("//span[@class='taLnk ulBlueLinks']")
    for l in more_links:
        try:
            l.click()
        except:
            pass
    html = browser.page_source
    soup = BeautifulSoup(html, "lxml")
    review_blocks = soup.find_all('div', 'reviewSelector')   
    for r in review_blocks:
        int_rating = int(r.find('span','ui_bubble_rating')['class'][1].split('_')[1])//10
        if int_rating > 3:
            rating = 'pos'
        else:
            rating = 'neg'
        review = r.p.text
        dataset.append((rating,review))

browser.quit()
print('Positive: ' + str(sum(1 for t in dataset if t[0] == 'pos')))
print('Negative: ' + str(sum(1 for t in dataset if t[0] == 'neg')))
characters_to_remove = ["''",'``',"rt","https","’","“","”","\u200b","--","n't","'s","...","//t.c"]
wordnet_lemmatizer = WordNetLemmatizer()


def bow_features(review):
    words = word_tokenize(review)
    words = [word.lower() for word in words]
    words = [word for word in words if word not in set(characters_to_remove)]
    words = [word for word in words if word not in stopwords.words("english")]
    words = [word for word in words if word not in set(string.punctuation[1:])]

    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in words]
    my_dict = dict([(word, True) for word in lemma_list])
    return my_dict
f_dataset =[(bow_features(dataset[i][1]), dataset[i][0]) for i in range(len(dataset))]
random.shuffle(f_dataset)
train_set, test_set = f_dataset[:int(len(f_dataset)*.75)], f_dataset[int(len(f_dataset)*.75):]
nb_classifier = nltk.NaiveBayesClassifier.train(train_set)
print(nltk.classify.accuracy(nb_classifier, test_set))
nb_classifier.show_most_informative_features(100)
refsets = collections.defaultdict(set)
testsets = collections.defaultdict(set)
 
for i, (feats, label) in enumerate(train_set):
    refsets[label].add(i)
    observed = nb_classifier.classify(feats)
    testsets[observed].add(i)
    
print('pos precision:', precision(refsets['pos'], testsets['pos']))
print('pos recall:', recall(refsets['pos'], testsets['pos']))
print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))
print('neg precision:', precision(refsets['neg'], testsets['neg']))
print('neg recall:', recall(refsets['neg'], testsets['neg']))
print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))
SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)

print(accuracy(SVM_classifier, test_set))

for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = dt_classifier.classify(feats)
    testsets[observed].add(i)

print('pos precision:', precision(refsets['pos'], testsets['pos']))
print('pos recall:', recall(refsets['pos'], testsets['pos']))
print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))
print('neg precision:', precision(refsets['neg'], testsets['neg']))
print('neg recall:', recall(refsets['neg'], testsets['neg']))
print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))
logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)

print(accuracy(logit_classifier, test_set))

for i, (feats, label) in enumerate(test_feats):
    refsets[label].add(i)
    observed = logit_classifier.classify(feats)
    testsets[observed].add(i)
dt_classifier = DecisionTreeClassifier.train(train_set, 
                                             binary=True, 
                                             entropy_cutoff=0.8, 
                                             depth_cutoff=5, 
                                             support_cutoff=30)
accuracy(dt_classifier, test_set)

for i, (feats, label) in enumerate(test_set):
    refsets[label].add(i)
    observed = dt_classifier.classify(feats)
    testsets[observed].add(i)
    
print('pos precision:', precision(refsets['pos'], testsets['pos']))
print('pos recall:', recall(refsets['pos'], testsets['pos']))
print('pos F-measure:', f_measure(refsets['pos'], testsets['pos']))
print('neg precision:', precision(refsets['neg'], testsets['neg']))
print('neg recall:', recall(refsets['neg'], testsets['neg']))
print('neg F-measure:', f_measure(refsets['neg'], testsets['neg']))
